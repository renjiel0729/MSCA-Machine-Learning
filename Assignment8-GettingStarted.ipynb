{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 8: Recurrent Neural Networks(RNN)\n",
    "NOTE : PLEASE DO NOT POST/SHARE THE CODE OR YOUR SOLUTIONS ON THE WEB/GIT except CANVAS FOR GRADING\n",
    "\n",
    "References:\n",
    "https://www.kaggle.com/c/nlp-getting-started/notebooks\n",
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "https://paperswithcode.com/method/bilstm\n",
    "https://towardsdatascience.com/sentiment-analysis-with-deep-learning-62d4d0166ef6\n",
    "https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Management Problem\n",
    "Executive management of a large retailer is thinking about using a language model to classify written customer reviews of call and complaint logs. The goal here is, if the most critical customer messages can be identified, then customer support personnel can be assigned to contact those customers to gather feedback and enhance customer experience.\n",
    "\n",
    "How would you advise senior management? What kinds of systems and methods would be most relevant to the customer services function? Considering the results of this assignment in particular, what is needed to make an automated customer support system that is capable of identifying negative customer feelings? What can data scientists do to make language models more useful in a customer service function?\n",
    "\n",
    "\n",
    "Description:\n",
    "This assignment involves working with language models developed with pretrained word vectors. We use sentences (sequences of words) to train language models for predicting movie review sentiment (thumbs up versus thumbs down). We study effects of word vector size, vocabulary size, and neural network structure (hyperparameters) on classification performance. We build on resources for recurrent neural networks (RNNs) as implemented in TensorFlow. RNNs are well suited to the analysis of sequences, as needed for natural language processing (NLP). \n",
    "\n",
    "Initial background reading for this assignment comes from Chapter 15/16 (pp. 495–567) of the Géron textbook:\n",
    "Géron, A. (2019). Hands-on machine learning with Scikit-Learn & TensorFlow: Concepts, tools, and techniques to build intelligent systems. Sebastopol, CA: O’Reilly. [ISBN-13 978-1-491-96229-9]. Source code available via Github (https://github.com/ageron/handson-ml2)\n",
    "\n",
    "Specialized RNN models have been developed to accommodate the needs of many language processing tasks. Larger relevant vocabularies are usually associated with more accurate models, but training with larger vocabularies requires more memory and longer processing times. We can speed up the training process by using pretrained word vectors and subsets of pretrained word vectors.\n",
    "\n",
    "Technologies such as word2vec, GloVe (global vectors), and fastText provide ways of representing words as numeric vectors. These numeric vectors or neural network embeddings capture the meaning of words as well as their common usage as parts of speech. Word embeddings have extensive applications in natural language processing.\n",
    "\n",
    "This assignment requires the use of two pretrained word embeddings selected from a list of supported vectors. That is, we replace each word in a sentence or sequence with a vector of numbers. Methods for downloading word embeddings are provided in the Python package chakin (https://github.com/chakki-works/chakin)\n",
    "\n",
    "Early work on word2vec embeddings is cited in these references:\n",
    "Mikolov, T., et al. (2013). Efficient Estimation of Word Representations in Vector Space (https://arxiv.org/pdf/1301.3781.pdf). \n",
    "Mikolov, T., et al. (2013). Linguistic Regularities in Continuous Space Word Representations (https://www.aclweb.org/anthology/N13-1090/). \n",
    "\n",
    "GloVe, a method for estimating pretrained word vectors, was developed at Stanford (https://nlp.stanford.edu/projects/glove/). A tutorial and code for using GloVe embeddings is available here. (https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer)\n",
    "\n",
    "A third set of pretrained vectors is fastText, described here (https://arxiv.org/pdf/1607.04606.pdf).  Word embeddings are an active area of research, as shown in recent developments in probabilistic fastText (https://www.aclweb.org/anthology/P18-1001/).\n",
    "\n",
    "We can also test vocabulary sizes associated with pretrained word vectors, defined as part of the language model. For example, we might compare a vocabulary of the top 10,000 words in English versus a vocabulary of the top 30,000 words.Additionally, we can test alternative RNN structures and settings of hyperparameters.\n",
    "\n",
    "\n",
    "Requirements:\n",
    "- Install the Python chakin package, obtain GloVe (and perhaps non-GloVe) embeddings.\n",
    "- Run the starter code below for the assignment, which uses pretrained word vectors from GloVe.6B.50d, a vocabulary of 10,000 words, and movie review data.\n",
    "- Revise the jump-start code to accommodate two pretrained word vectors and two vocabulary sizes. These represent the cells of a completely crossed 2-by-2 experimental design, defining four distinct language models.\n",
    "- Build and evaluate at least four language models of the experimental design. For each cell in the design, compute classification accuracy in the test set.\n",
    "- Evaluate the four language models and make recommendations to management.\n",
    "- Test two or more alternative RNN structures or hyperparameter settings.\n",
    "\n",
    "\n",
    "Deliverables and File Formats\n",
    "- Python notebook that address the problem and the writeup as indicated towards the end of this notebook (Audience:Director Data Science/Analytics)\n",
    "\n",
    "Optional (Audience:Business/C-Suite) - Additional 20 points\n",
    "1. Provide a double-spaced paper with a two-page maximum for the text. The paper in pdf format should include \n",
    "    (1) Summary and problem definition for management; \n",
    "    (2) Discussion of the methodology, data findings and traditional machine learning methods employed; \n",
    "    (3) List assumptions, programming work, issues along with model evaluation metrics; and \n",
    "    (4) Review of results/ insight swith recommendations for management.\n",
    "\n",
    "Formatting Python Code\n",
    "Refer to Google’s Python Style Guide (https://google.github.iostyleguide/pyguide.html) for ideas about formatting Python code:\n",
    "\n",
    "\n",
    "NOTE : \n",
    "- Below is the starter code and please feel free to update/edit/change to provide your thoughts/solutions to the problem. \n",
    "- Comment often and in detail, highlighting major sections of code, describing the thinking behind the programming methods being employed.\n",
    "- This code has a lot of errors so please make sure to updated all the cells based on best practices along with your analysis/findings.\n",
    "\n",
    "\n",
    "GRADING GUIDELINES (100 points)\n",
    "--------------------------------\n",
    "(1) Data preparation, exploration, visualization (20 points)\n",
    "(2) Review research design and modeling methods (20 points)\n",
    "(3) Review results, evaluate models (20 points)\n",
    "(4) Implementation and programming (20 points)\n",
    "(5) Exposition, problem description, and management recommendations (20 points) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chakin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# S1 Install & Import Packages \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import sklearn\n",
    "\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "\n",
    "import chakin  \n",
    "import json\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from itertools import chain\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd  # data frame operations  \n",
    "import numpy as np  # arrays and math functions\n",
    "import matplotlib.pyplot as plt  # static plotting\n",
    "import re # regular expressions\n",
    "import scipy\n",
    "import os # Operation System\n",
    "import os.path\n",
    "import time # Record processing time\n",
    "\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S2 Mount Google Drive to Colab Enviorment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S3 Establish working directory - COlAB\n",
    "# Need to create this directory structure on Goolge Drive\n",
    "os.getcwd()\n",
    "%cd /content/gdrive/My Drive/wk8/\n",
    "!pwd\n",
    "!ls\n",
    "print('Working Directory')\n",
    "print(os.getcwd())\n",
    "work_dir = \"/content/gdrive/My Drive/wk8/\"\n",
    "data_dir = work_dir+\"/data/\"\n",
    "chp_id = \"rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory\n",
      "C:\\Users\\sbhar\\Desktop\\shree\\teaching\\08-MLPA\\03-Assignments\\8\n",
      "./data/train/\n"
     ]
    }
   ],
   "source": [
    "#S3 Establish working directory  - LOCAL\n",
    "print('Working Directory')\n",
    "print(os.getcwd())\n",
    "\n",
    "work_dir = \"./\"\n",
    "data_dir = work_dir +\"data/\"\n",
    "chp_id = \"rnn\"\n",
    "print(data_dir +\"train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "# Import data and Word Embeddings for Sentiment analysis\n",
    "chakin.search(lang='English')  # lists available indices in English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collection\n",
    "##### RNN - Movie Review Sentiment  PART 1 : run chakin to get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify English embeddings file to download and install by index number, number of dimensions, and subfoder name\n",
    "# Note that GloVe 50-, 100-, 200-, and 300-dimensional folders are downloaded with a single zip download\n",
    "CHAKIN_INDEX = 11\n",
    "NUMBER_OF_DIMENSIONS = 50\n",
    "SUBFOLDER_NAME = \"gloVe.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading embeddings to 'embeddings\\gloVe.6B.zip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100% ||                                      | Time:  0:02:39   5.1 MiB/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings to 'embeddings\\gloVe.6B'\n",
      "\n",
      "Run complete\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"embeddings\"\n",
    "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
    "if SUBFOLDER_NAME[-1] == \"d\":\n",
    "    GLOVE_FILENAME = os.path.join(\n",
    "        UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
    "else:\n",
    "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(\n",
    "        SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
    "\n",
    "\n",
    "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
    "    # GloVe by Stanford is licensed Apache 2.0:\n",
    "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
    "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
    "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
    "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
    "else:\n",
    "    print(\"Embeddings already downloaded.\")\n",
    "\n",
    "if not os.path.exists(UNZIP_FOLDER):\n",
    "    import zipfile\n",
    "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
    "        ZIP_FILE = ZIP_FILE_ALT\n",
    "    with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
    "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
    "        zip_ref.extractall(UNZIP_FOLDER)\n",
    "else:\n",
    "    print(\"Embeddings already extracted.\")\n",
    "\n",
    "print('\\nRun complete')\n",
    "\n",
    "# After this step there should be\n",
    "# embeddings folder in the current working directory A\n",
    "# Directory called glove.6b within embeddings directory\n",
    "# 4 files within it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation & Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed value for random number generators to obtain reproducible results\n",
    "RANDOM_SEED = 9999\n",
    "\n",
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary \n",
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for loading embeddings follows methods described in\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
    "# for the requested pre-trained word embeddings\n",
    "\n",
    "# Note the use of defaultdict data structure from the Python Standard Library\n",
    "# collections_defaultdict.py lets the caller specify a default value up front\n",
    "# The default value will be retuned if the key is not a known dictionary key\n",
    "# That is, unknown words are represented by a vector of zeros\n",
    "# For word embeddings, this default value is a vector of zeros\n",
    "# Documentation for the Python standard library:\n",
    "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
    "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
    "\n",
    "\n",
    "# Load a embedding text file\n",
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, we return a tuple of two dictionaries \n",
    "    `(word_to_index_dict, index_to_embedding_array)`, otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionary mapping from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "            split = line.split(' ')\n",
    "            word = split[0]\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "# Check if the loaded embedding files glove.6B.50d. successfully.   \"glove.6B.50d.\"was ontained though \"run-chakin-to-get-embeddings-v001.py\"\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")\n",
    "\n",
    "# Note: unknown words have representations with values [0, 0, ..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is of shape: (400001, 50)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n"
     ]
    }
   ],
   "source": [
    "# Additional background code from\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# shows the general structure of the data structures for word embeddings\n",
    "# This code is modified for our purposes in language modeling \n",
    "\n",
    "# Check vocabrary size and embedding dimention\n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "#Check embedding data\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 10000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# code for working with movie reviews data \n",
    "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
    "#    Upper Saddle River, N.J.: Pearson Education.\n",
    "#    ISBN-13: 978-0-13-388644-3\n",
    "# This original study used a simple bag-of-words approach\n",
    "# to sentiment analysis, along with pre-defined lists of\n",
    "# negative and positive words.        \n",
    "# Code available at:  https://github.com/mtpa/wnds       \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text parsing function for creating text documents \n",
    "# there is more we could do for data preparation \n",
    "# stemming... looking for contractions... possessives... \n",
    "# but we will work with what we have in this parsing function\n",
    "# if we want to do stemming at a later time, we can use\n",
    "#     porter = nltk.PorterStemmer()  \n",
    "# in a construction like this\n",
    "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: C:\\Users\\sbhar\\Desktop\\shree\\teaching\\08-MLPA\\03-Assignments\\8./data/movie-reviews-negative\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# Download and install both negtive and positive movies reviews from the URL below\n",
    "# https://github.com/mtpa/wnds/tree/master/WNDS_Chapter_8\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Load Negative Movie reviews in lists of lists\n",
    "# Create /movie-reviews-negative directory and move negative IMDB files to this drive\n",
    "# os.mkdir(work_dir+'movie-reviews-negative')\n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Set path to the negative word dictionary, \"moive-reviews-negative\"\n",
    "dir_name = os.getcwd() + './data/movie-reviews-negative'\n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under C:\\Users\\sbhar\\Desktop\\shree\\teaching\\08-MLPA\\03-Assignments\\8./data/movie-reviews-negative\n",
      "Data size (Characters) (Document 499) 133\n",
      "Sample string (Document 499) ['this', 'is', 'one', 'of', 'the', 'dumbest', 'films', 've', 'ever', 'seen', 'it', 'rips', 'off', 'nearly', 'ever', 'type', 'of', 'thriller', 'and', 'manages', 'to', 'make', 'mess', 'of', 'them', 'all', 'br', 'br', 'there', 'not', 'single', 'good', 'line', 'or', 'character', 'in', 'the', 'whole', 'mess', 'if', 'there', 'was', 'plot', 'it', 'was', 'an', 'afterthought', 'and', 'as', 'far']\n"
     ]
    }
   ],
   "source": [
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        data = tf.compat.as_str(f.read())\n",
    "        data = data.lower()\n",
    "        data = text_parse(data)\n",
    "        data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "    return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    \n",
    "print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "print('Sample string (Document %d) %s'%(i,words[:50]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: ./data/movie-reviews-positive\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Set path to the positive word dictionary, \"moive-reviews-positive\"\n",
    "dir_name = './data/movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under ./data/movie-reviews-positive\n",
      "Data size (Characters) (Document 499) 157\n",
      "Sample string (Document 499) ['working', 'class', 'romantic', 'drama', 'from', 'director', 'martin', 'ritt', 'is', 'as', 'unbelievable', 'as', 'they', 'come', 'yet', 'there', 'are', 'moments', 'of', 'pleasure', 'due', 'mostly', 'to', 'the', 'charisma', 'of', 'stars', 'jane', 'fonda', 'and', 'robert', 'de', 'niro', 'both', 'terrific', 'she', 'widow', 'who', 'can', 'move', 'on', 'he', 'illiterate', 'and', 'closet', 'inventor', 'you', 'can', 'probably', 'guess']\n"
     ]
    }
   ],
   "source": [
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        data = tf.compat.as_str(f.read())\n",
    "        data = data.lower()\n",
    "        data = text_parse(data)\n",
    "        data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "    return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "    positive_documents.append(words)\n",
    "    \n",
    "print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "print('Sample string (Document %d) %s'%(i,words[:50]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 0\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "        embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: story\n",
      "Embedding for this word:\n",
      " [ 0.48251    0.87746   -0.23455    0.0262     0.79691    0.43102\n",
      " -0.60902   -0.60764   -0.42812   -0.012523  -1.2894     0.52656\n",
      " -0.82763    0.30689    1.1972    -0.47674   -0.46885   -0.19524\n",
      " -0.28403    0.35237    0.45536    0.76853    0.0062157  0.55421\n",
      "  1.0006    -1.3973    -1.6894     0.30003    0.60678   -0.46044\n",
      "  2.5961    -1.2178     0.28747   -0.46175   -0.25943    0.38209\n",
      " -0.28312   -0.47642   -0.059444  -0.59202    0.25613    0.21306\n",
      " -0.016129  -0.29873   -0.19468    0.53611    0.75459   -0.4112\n",
      "  0.23625    0.26451  ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.48251    0.87746   -0.23455    0.0262     0.79691    0.43102\n",
      " -0.60902   -0.60764   -0.42812   -0.012523  -1.2894     0.52656\n",
      " -0.82763    0.30689    1.1972    -0.47674   -0.46885   -0.19524\n",
      " -0.28403    0.35237    0.45536    0.76853    0.0062157  0.55421\n",
      "  1.0006    -1.3973    -1.6894     0.30003    0.60678   -0.46044\n",
      "  2.5961    -1.2178     0.28747   -0.46175   -0.25943    0.38209\n",
      " -0.28312   -0.47642   -0.059444  -0.59202    0.25613    0.21306\n",
      " -0.016129  -0.29873   -0.19468    0.53611    0.75459   -0.4112\n",
      "  0.23625    0.26451  ]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[0][0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: but\n",
      "Embedding for this word:\n",
      " [ 0.35934   -0.2657    -0.046477  -0.2496     0.54676    0.25924\n",
      " -0.64458    0.1736    -0.53056    0.13942    0.062324   0.18459\n",
      " -0.75495   -0.19569    0.70799    0.44759    0.27031   -0.32885\n",
      " -0.38891   -0.61606   -0.484      0.41703    0.34794   -0.19706\n",
      "  0.40734   -2.1488    -0.24284    0.33809    0.43993   -0.21616\n",
      "  3.7635     0.19002   -0.12503   -0.38228    0.12944   -0.18272\n",
      "  0.076803   0.51579    0.0072516 -0.29192   -0.27523    0.40593\n",
      " -0.040394   0.28353   -0.024724   0.10563   -0.32879    0.10673\n",
      " -0.11503    0.074678 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.35934   -0.2657    -0.046477  -0.2496     0.54676    0.25924\n",
      " -0.64458    0.1736    -0.53056    0.13942    0.062324   0.18459\n",
      " -0.75495   -0.19569    0.70799    0.44759    0.27031   -0.32885\n",
      " -0.38891   -0.61606   -0.484      0.41703    0.34794   -0.19706\n",
      "  0.40734   -2.1488    -0.24284    0.33809    0.43993   -0.21616\n",
      "  3.7635     0.19002   -0.12503   -0.38228    0.12944   -0.18272\n",
      "  0.076803   0.51579    0.0072516 -0.29192   -0.27523    0.40593\n",
      " -0.040394   0.28353   -0.024724   0.10563   -0.32879    0.10673\n",
      " -0.11503    0.074678 ]\n"
     ]
    }
   ],
   "source": [
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[6][9][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: from\n",
      "Embedding for this word:\n",
      " [ 0.41037   0.11342   0.051524 -0.53833  -0.12913   0.22247  -0.9494\n",
      " -0.18963  -0.36623  -0.067011  0.19356  -0.33044   0.11615  -0.58585\n",
      "  0.36106   0.12555  -0.3581   -0.023201 -1.2319    0.23383   0.71256\n",
      "  0.14824   0.50874  -0.12313  -0.20353  -1.82      0.22291   0.020291\n",
      " -0.081743 -0.27481   3.7343   -0.01874  -0.084522 -0.30364   0.27959\n",
      "  0.043328 -0.24621   0.015373  0.49751   0.15108  -0.01619   0.40132\n",
      "  0.23067  -0.10743  -0.36625  -0.051135  0.041474 -0.36064  -0.19616\n",
      " -0.81066 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.41037   0.11342   0.051524 -0.53833  -0.12913   0.22247  -0.9494\n",
      " -0.18963  -0.36623  -0.067011  0.19356  -0.33044   0.11615  -0.58585\n",
      "  0.36106   0.12555  -0.3581   -0.023201 -1.2319    0.23383   0.71256\n",
      "  0.14824   0.50874  -0.12313  -0.20353  -1.82      0.22291   0.020291\n",
      " -0.081743 -0.27481   3.7343   -0.01874  -0.084522 -0.30364   0.27959\n",
      "  0.043328 -0.24621   0.015373  0.49751   0.15108  -0.01619   0.40132\n",
      "  0.23067  -0.10743  -0.36625  -0.051135  0.041474 -0.36064  -0.19616\n",
      " -0.81066 ]\n"
     ]
    }
   ],
   "source": [
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[999][39][:])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Apply embeddings to numpy\n",
    "embeddings_array = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training data:  (800,)\n",
      "Shape of Test data:  (200,)\n",
      "\n",
      "Shape of Training data:  (800,)\n",
      "Shape of Test data:  (200,)\n"
     ]
    }
   ],
   "source": [
    "# Set training and test data\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "\n",
    "print(\"Shape of Training data: \", X_train.shape)\n",
    "print(\"Shape of Test data: \", X_test.shape)\n",
    "\n",
    "print(\"\\nShape of Training data: \", y_train.shape)\n",
    "print(\"Shape of Test data: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Model: Simple RNN with BPTT\n",
    "##### GloVe.6B.50d - Embedding Vocabulary Size 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code for base case\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001 # Learning rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execution Phase###\n",
    "\n",
    "# Set number of epochs and batch size for training model.\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "# Record start time for neural network training\n",
    "start_time_base = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction classes and actual classes.\n",
    "print(\"-------- Base Model --------\")\n",
    "print(\"RNN backpropagation through time (BPTT)\")\n",
    "print(\"\\nPredicted classes:\", y_pred_base)\n",
    "print(\"Actual classes:\", y_test[:25])\n",
    "print(\"Test Set Accuracy:\", accuracy_base)\n",
    "print(\"Train Set Accuracy:\", accuracy_base_y)\n",
    "# Record end time for neural network training\n",
    "stop_time_base = time.clock()\n",
    "\n",
    "#Total processing time\n",
    "runtime_base = stop_time_base - start_time_base \n",
    "\n",
    "print(\"\\nStart time:\", start_time_base)\n",
    "print(\"Stop time:\", stop_time_base)\n",
    "print(\"processing time:\", runtime_base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN with LSTM cells and 3 Layers\n",
    "#### GloVe.6B.50d - Embedding Vocabulary Size 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Input, Add\n",
    "from keras.layers import Input, LSTM, GRU\n",
    "from keras.models import Sequential, Model\n",
    "def generateModel1():\n",
    "    inputLayer=Input(shape=(40,50))\n",
    "    unitNum = 32\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(inputLayer)\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=LSTM(units=unitNum, activation='elu')(model)\n",
    "    outputModel=Dense(1)(model)\n",
    "    model=Model(inputLayer,outputModel)\n",
    "    return model\n",
    "start_time_base_1 = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 40, 50)]          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 40, 32)            10624     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 27,297\n",
      "Trainable params: 27,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = generateModel1()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import adam\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "lossThreshold = .1\n",
    "valLossThreshold = .1\n",
    "count = 0\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_1 = model.evaluate(X_test, y_test, verbose = 0)\n",
    "accuracy_base_y_1 = model.evaluate(X_train, y_train,verbose = 0)\n",
    "stop_time_base_1 = time.clock()\n",
    "time_1 = stop_time_base_1 - start_time_base_1\n",
    "print(\"processing time\")\n",
    "print(\"Test Set Accuracy:\", accuracy_base_y_1[1])\n",
    "print(\"Train Set Accuracy:\", accuracy_1[1])\n",
    "print(\"processing time\", time_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: RNN with LSTM cells and 3 Layers\n",
    "#### GloVe.6B.50d - Embedding Vocabulary Size 30,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 8: RNN with GRU cells and 5 Layers\n",
    "#### GloVe.6B.100d - Embedding Vocabulary Size 30,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_base_8 = time.clock()\n",
    "def generateModel8():\n",
    "    inputLayer=Input(shape=(40,50))\n",
    "    unitNum = 32\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(inputLayer)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, activation='elu')(model)\n",
    "    outputModel=Dense(1)(model)\n",
    "    model=Model(inputLayer,outputModel)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8 = generateModel8()\n",
    "print(model_8.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "lossThreshold = .1\n",
    "valLossThreshold = .1\n",
    "\n",
    "hist_8 = model_8.fit(X1_train, y1_train, validation_data=(X1_test, y1_test), epochs=50, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_8 = model_8.evaluate(X1_test, y1_test, verbose = 0)\n",
    "accuracy_base_y_8 = model_8.evaluate(X1_train, y1_train)\n",
    "stop_time_base_8 = time.clock()\n",
    "time_8 = stop_time_base_8 - start_time_base_8\n",
    "\n",
    "print(\"Test Set Accuracy:\", accuracy_base_y_8[1])\n",
    "print(\"Train Set Accuracy:\",accuracy_8[1] )\n",
    "print(\"processing time\", time_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Summary Table\n",
    "from tabulate import tabulate\n",
    "\n",
    "col_labels = ['# - Model', 'Number of Layers', 'Embedding Vocabulary Size','Processing Time', 'Test Set Accuracy', 'Training Set Accuracy']\n",
    "\n",
    "table_vals = [['1 - Simple RNN with BPTT','3','10000',runtime_base, accuracy_base,accuracy_base_y],\n",
    "              ['2 - RNN with LSTM cells','3','10000',time_1,accuracy_1[1],accuracy_base_y_1[1]],\n",
    "              ['3 - RNN with LSTM cells','3','30000',time_2,accuracy_2[1],accuracy_base_y_2[1]],\n",
    "              ['4 - RNN with LSTM cells','5','10000',time_3,accuracy_3[1],accuracy_base_y_3[1]],\n",
    "              ['5 - RNN with LSTM cells','5','30000',time_4,accuracy_4[1],accuracy_base_y_4[1]],\n",
    "              ['6 - RNN with GRU','3','10000',time_5,accuracy_5[1],accuracy_base_y_5[1]],\n",
    "              ['7 - RNN with GRU','3','30000',time_6,accuracy_6[1],accuracy_base_y_6[1]],\n",
    "              ['8 - RNN with GRU','5','10000',time_7,accuracy_7[1],accuracy_base_y_7[1]],\n",
    "              ['9 - RNN with GRU','5','30000',time_8,accuracy_8[1],accuracy_base_y_8[1]],]\n",
    "\n",
    "print('------------------------------- Summary Table -------------------------------')\n",
    "\n",
    "table = tabulate(table_vals, headers=col_labels, tablefmt=\"simple\",numalign=\"left\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Management Problem\n",
    "Executive management of a large retailer is thinking about using a language model to classify written customer reviews of call and complaint logs. The goal here is, if the most critical customer messages can be identified, then customer support personnel can be assigned to contact those customers to gather feedback and enhance customer experience.\n",
    "- How would you advise senior management? What kinds of systems and methods would be most relevant to the customer services function? Considering the results of this assignment in particular, what is needed to make an automated customer support system that is capable of identifying negative customer feelings? What can data scientists do to make language models more useful in a customer service function?\n",
    "\n",
    "\n",
    "#### REPORT/FINDINGS: \n",
    "(1) A summary and problem definition for management; \n",
    "\n",
    "(2) Discussion of the research design, measurement and statistical methods, traditional and machine learning methods employed \n",
    "\n",
    "(3) Overview of programming work; \n",
    "\n",
    "(4) Review of results with recommendations for management."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_rnn.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
